# Module 4: Vision-Language-Action (VLA)

This module covers Vision-Language-Action systems that connect perception, language understanding, and robotic action execution. It includes OpenAI Whisper for voice commands, LLM-driven cognitive planning, and integration with ROS 2 actions.

## Learning Objectives

By the end of this module, you will be able to:
- Integrate OpenAI Whisper for voice command recognition
- Implement LLM-driven cognitive planning for natural language understanding
- Map natural language commands to ROS 2 actions
- Build complete VLA pipelines for robot interaction
- Create the capstone autonomous humanoid system

## Topics Covered

1. [OpenAI Whisper: Voice Command Processing](./whisper.md)
2. [LLM-Driven Cognitive Planning](./llm-planning.md)
3. [Natural Language to ROS 2 Actions](./nlp-ros.md)
4. [Capstone: Autonomous Humanoid Integration](./capstone.md)